import{s as le,a as S,e as G,n as ne}from"../chunks/scheduler.FGUOCI5T.js";import{S as ae,i as se,k as Q,l as U,m as X,n as Y,o as ee,p as te,e as o,s as p,c as r,q as m,f as c,a as ie,d as n,r as oe,g as a}from"../chunks/index.CPkGT8D8.js";import{P as re,g as ue,a as J}from"../chunks/post_layout.BO4JL1Bi.js";import{I as pe}from"../chunks/post_card.CgyAYwQO.js";function me(T){let l,d="ğ—¦ğ˜‚ğ—½ğ—²ğ—¿ğ—°ğ—µğ—®ğ—¿ğ—´ğ—¶ğ—»ğ—´ ğ—”ğ˜€ğ˜†ğ—»ğ—° ğ—¶ğ—» ğ—–#: ğ—–ğ—¼ğ—ºğ—½ğ—®ğ—¿ğ—² ğ—¦ğ—²ğ—¾ğ˜‚ğ—²ğ—»ğ˜ğ—¶ğ—®ğ—¹ ğ˜ƒğ˜€. ğ—£ğ—®ğ—¿ğ—®ğ—¹ğ—¹ğ—²ğ—¹ ğ˜„ğ—¶ğ˜ğ—µ ğ—§ğ—®ğ˜€ğ—¸.ğ—ªğ—µğ—²ğ—»ğ—”ğ—¹ğ—¹ âš¡",f,u,s=`Hey Devs! ğŸ‘‹
Have you ever wanted to call multiple services or run multiple tasks at once without waiting for each one to finish before starting the next? Thatâ€™s exactly where ğ—§ğ—®ğ˜€ğ—¸.ğ—ªğ—µğ—²ğ—»ğ—”ğ—¹ğ—¹ comes to the rescue! Instead of running each task one by one (which can be slow if each task takes a while), you can fire them all simultaneously, wait for all to complete, and then process the results in bulk.`,i,h,B="Below, we have a ğ˜‰ğ˜¦ğ˜¯ğ˜¤ğ˜©ğ˜®ğ˜¢ğ˜³ğ˜¬ğ˜‹ğ˜°ğ˜µğ˜•ğ˜¦ğ˜µ setup that measures:",P,v,D="<li>ğ—¦ğ—²ğ—¾ğ˜‚ğ—²ğ—»ğ˜ğ—¶ğ—®ğ—¹ ğ—¿ğ—²ğ—¾ğ˜‚ğ—²ğ˜€ğ˜ğ˜€ (one after another).</li> <li>ğ—£ğ—®ğ—¿ğ—®ğ—¹ğ—¹ğ—²ğ—¹ ğ—¿ğ—²ğ—¾ğ˜‚ğ—²ğ˜€ğ˜ğ˜€ (all at once using ğ˜›ğ˜¢ğ˜´ğ˜¬.ğ˜ğ˜©ğ˜¦ğ˜¯ğ˜ˆğ˜­ğ˜­).</li>",L,w,V=`By measuring both approaches, weâ€™ll see how ğ˜›ğ˜¢ğ˜´ğ˜¬.ğ˜ğ˜©ğ˜¦ğ˜¯ğ˜ˆğ˜­ğ˜­ can significantly reduce the overall time when dealing with I/O-bound tasks such as HTTP calls.
Letâ€™s dive in! ğŸŠâ€â™‚ï¸`,H,x,W="ğ—ªğ—µğ˜† ğ˜‚ğ˜€ğ—² ğ™ğ™–ğ™¨ğ™ .ğ™’ğ™ğ™šğ™£ğ˜¼ğ™¡ğ™¡? ğŸ’¡",q,y,Z=`<li><p>ğ—§ğ—¶ğ—ºğ—² ğ—¦ğ—®ğ˜ƒğ—¶ğ—»ğ—´ğ˜€
If each request takes 500 ms, and you have 10 requests, a sequential approach might end up taking ~5000 ms. With ğ˜›ğ˜¢ğ˜´ğ˜¬.ğ˜ğ˜©ğ˜¦ğ˜¯ğ˜ˆğ˜­ğ˜­, it could take around 500 ms total (assuming all 10 run simultaneously)!</p></li> <li><p>ğ—–ğ—¹ğ—²ğ—®ğ—»ğ—²ğ—¿ ğ—–ğ—¼ğ—±ğ—²
Instead of a big ğ˜§ğ˜°ğ˜³ğ˜¦ğ˜¢ğ˜¤ğ˜© that awaits every single call, you collect all tasks in a list and await them together. Easy to read and maintain!</p></li> <li><p>ğ—•ğ—²ğ˜ğ˜ğ—²ğ—¿ ğ—¥ğ—²ğ˜€ğ—¼ğ˜‚ğ—¿ğ—°ğ—² ğ—¨ğ˜ğ—¶ğ—¹ğ—¶ğ˜‡ğ—®ğ˜ğ—¶ğ—¼ğ—»
While one request is waiting on network I/O, others can proceed. This helps maximize concurrency without writing complex multi-threaded logic.</p></li>`,I,g,A=`ğ—˜ğ˜…ğ—½ğ—²ğ—°ğ˜ğ—²ğ—± ğ—¢ğ˜‚ğ˜ğ—°ğ—¼ğ—ºğ—² & ğ—§ğ—®ğ—¸ğ—²ğ—®ğ˜„ğ—®ğ˜†ğ˜€ ğŸ¤”
ğ—¦ğ—²ğ—¾ğ˜‚ğ—²ğ—»ğ˜ğ—¶ğ—®ğ—¹ (ğ˜ğ˜¦ğ˜µğ˜ˆğ˜­ğ˜­ğ˜šğ˜¦ğ˜²ğ˜¶ğ˜¦ğ˜¯ğ˜µğ˜ªğ˜¢ğ˜­ğ˜ˆğ˜´ğ˜ºğ˜¯ğ˜¤): Runs each HTTP-like call one by one, typically summing all delays (e.g., 10 Ã— 500 ms).`,M,C,F="ğ—£ğ—®ğ—¿ğ—®ğ—¹ğ—¹ğ—²ğ—¹ (ğ˜ğ˜¦ğ˜µğ˜ˆğ˜­ğ˜­ğ˜—ğ˜¢ğ˜³ğ˜¢ğ˜­ğ˜­ğ˜¦ğ˜­ğ˜ˆğ˜´ğ˜ºğ˜¯ğ˜¤): Initiates all tasks at once and waits for them together, drastically reducing total time if each call is I/O-bound.",O,k,N="Feel free to share your thoughts or experiences with parallel async calls! Do you have any real-world story of boosting performance with ğ˜›ğ˜¢ğ˜´ğ˜¬.ğ˜ğ˜©ğ˜¦ğ˜¯ğ˜ˆğ˜­ğ˜­? Let us know in the comments! ğŸ‰",j,b,$,z,_,R='<a href="#view-on-linkedin"><a href="https://www.linkedin.com/feed/update/urn:li:activity:7280699390116155393/" rel="nofollow noopener noreferrer external" target="_blank">View on Linkedin</a></a>',E;return $=new pe({props:{src:"urara.png",alt:"image description"}}),{c(){l=o("p"),l.textContent=d,f=p(),u=o("p"),u.textContent=s,i=p(),h=o("p"),h.textContent=B,P=p(),v=o("ol"),v.innerHTML=D,L=p(),w=o("p"),w.textContent=V,H=p(),x=o("p"),x.textContent=W,q=p(),y=o("ol"),y.innerHTML=Z,I=p(),g=o("p"),g.textContent=A,M=p(),C=o("p"),C.textContent=F,O=p(),k=o("p"),k.textContent=N,j=p(),b=o("p"),Q($.$$.fragment),z=p(),_=o("h1"),_.innerHTML=R,this.h()},l(e){l=r(e,"P",{"data-svelte-h":!0}),m(l)!=="svelte-1ltbzia"&&(l.textContent=d),f=c(e),u=r(e,"P",{"data-svelte-h":!0}),m(u)!=="svelte-1vujy3u"&&(u.textContent=s),i=c(e),h=r(e,"P",{"data-svelte-h":!0}),m(h)!=="svelte-1qro8as"&&(h.textContent=B),P=c(e),v=r(e,"OL",{"data-svelte-h":!0}),m(v)!=="svelte-zwj7ra"&&(v.innerHTML=D),L=c(e),w=r(e,"P",{"data-svelte-h":!0}),m(w)!=="svelte-hrwwxt"&&(w.textContent=V),H=c(e),x=r(e,"P",{"data-svelte-h":!0}),m(x)!=="svelte-1gtllq6"&&(x.textContent=W),q=c(e),y=r(e,"OL",{"data-svelte-h":!0}),m(y)!=="svelte-17jq5kl"&&(y.innerHTML=Z),I=c(e),g=r(e,"P",{"data-svelte-h":!0}),m(g)!=="svelte-zd0qx0"&&(g.textContent=A),M=c(e),C=r(e,"P",{"data-svelte-h":!0}),m(C)!=="svelte-12nyg0x"&&(C.textContent=F),O=c(e),k=r(e,"P",{"data-svelte-h":!0}),m(k)!=="svelte-7dyhmr"&&(k.textContent=N),j=c(e),b=r(e,"P",{});var t=ie(b);U($.$$.fragment,t),t.forEach(n),z=c(e),_=r(e,"H1",{id:!0,"data-svelte-h":!0}),m(_)!=="svelte-uw9tkb"&&(_.innerHTML=R),this.h()},h(){oe(_,"id","view-on-linkedin")},m(e,t){a(e,l,t),a(e,f,t),a(e,u,t),a(e,i,t),a(e,h,t),a(e,P,t),a(e,v,t),a(e,L,t),a(e,w,t),a(e,H,t),a(e,x,t),a(e,q,t),a(e,y,t),a(e,I,t),a(e,g,t),a(e,M,t),a(e,C,t),a(e,O,t),a(e,k,t),a(e,j,t),a(e,b,t),X($,b,null),a(e,z,t),a(e,_,t),E=!0},p:ne,i(e){E||(Y($.$$.fragment,e),E=!0)},o(e){ee($.$$.fragment,e),E=!1},d(e){e&&(n(l),n(f),n(u),n(i),n(h),n(P),n(v),n(L),n(w),n(H),n(x),n(q),n(y),n(I),n(g),n(M),n(C),n(O),n(k),n(j),n(b),n(z),n(_)),te($)}}}function ce(T){let l,d;const f=[T[0],K];let u={$$slots:{default:[me]},$$scope:{ctx:T}};for(let s=0;s<f.length;s+=1)u=S(u,f[s]);return l=new re({props:u}),{c(){Q(l.$$.fragment)},l(s){U(l.$$.fragment,s)},m(s,i){X(l,s,i),d=!0},p(s,[i]){const h=i&1?ue(f,[i&1&&J(s[0]),i&0&&J(K)]):{};i&2&&(h.$$scope={dirty:i,ctx:s}),l.$set(h)},i(s){d||(Y(l.$$.fragment,s),d=!0)},o(s){ee(l.$$.fragment,s),d=!1},d(s){te(l,s)}}}const K={title:"ğ—–ğ—¼ğ—ºğ—½ğ—®ğ—¿ğ—² ğ—¦ğ—²ğ—¾ğ˜‚ğ—²ğ—»ğ˜ğ—¶ğ—®ğ—¹ ğ˜ƒğ˜€. ğ—£ğ—®ğ—¿ğ—®ğ—¹ğ—¹ğ—²ğ—¹ ğ˜„ğ—¶ğ˜ğ—µ ğ—§ğ—®ğ˜€ğ—¸.ğ—ªğ—µğ—²ğ—»ğ—”ğ—¹ğ—¹",created:"2025-01-02T00:00:00.000Z",tags:["Async",".NET","CSharp"],updated:"2025-01-03T00:58:45.987Z",images:[],slug:"/posts/task-when-all/+page.svelte.md",path:"/posts/task-when-all",toc:[{depth:1,slug:"view-on-linkedin",title:"View on Linkedin"}]};function fe(T,l,d){return T.$$set=f=>{d(0,l=S(S({},l),G(f)))},l=G(l),[l]}class we extends ae{constructor(l){super(),se(this,l,fe,ce,le,{})}}export{we as component};
